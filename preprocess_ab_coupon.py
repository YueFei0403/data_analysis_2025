import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy import stats
import sys
import os
from datetime import datetime

# --- Logging setup ---
log_dir = "./logs"
os.makedirs(log_dir, exist_ok=True)
logfile = os.path.join(log_dir, f"preprocess_{datetime.now().strftime('%Y%m%d_%H%M')}.log")
class TeeLogger(object):
    def __init__(self, filename):
        self.terminal = sys.stdout
        self.log = open(filename, "w", encoding="utf-8")
    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
    def flush(self):
        self.terminal.flush()
        self.log.flush()
sys.stdout = TeeLogger(logfile)
print(f"ğŸ“œ Logging all output to {logfile}")


# ==============================
# è¾…åŠ©å‡½æ•°
# ==============================
def check_users(df, stage, users_before=None):
    """æ‰“å°å½“å‰é˜¶æ®µæ•°æ®è§„æ¨¡ä¸ç”¨æˆ·å˜åŒ–"""
    n_rows = len(df)
    n_users = df['userid'].nunique()
    print(f"ğŸ§¾ {stage}: {n_rows} è¡Œ, {n_users} å”¯ä¸€ç”¨æˆ·")
    if users_before is not None:
        lost = users_before - set(df['userid'])
        print(f"   â†³ æœ¬é˜¶æ®µä¸¢å¤± {len(lost)} ä¸ªç”¨æˆ·")
        if len(lost) > 0:
            pd.DataFrame({'lost_userid': list(lost)}).to_csv(f'./lost_users_{stage}.csv', index=False)
            print(f"   ğŸ“ å·²å¯¼å‡º: ./lost_users_{stage}.csv")
    return set(df['userid'])

# ==============================
# 1. è¯»å–æ•°æ®ä¸åˆ—åæ ‡å‡†åŒ–
# ==============================
df = pd.read_csv('./user_data.csv')
df.columns = [c.strip().lower() for c in df.columns]

users_before = check_users(df, "åŸå§‹æ•°æ®è¯»å–")

# ==============================
# 2. userid å”¯ä¸€æ€§æ£€æŸ¥
# ==============================
dup_mask = df.duplicated(subset=['userid'], keep=False)
if dup_mask.any():
    print(f"âš ï¸ å‘ç° {dup_mask.sum()} æ¡é‡å¤è®°å½• ({df.loc[dup_mask,'userid'].nunique()} ç”¨æˆ·)")
    dup_users = df.loc[dup_mask, 'userid'].unique()
    print("ä»¥ä¸‹ä¸ºéƒ¨åˆ†é‡å¤ç”¨æˆ·:", dup_users[:10])
    print(df.loc[dup_mask, ['userid', 'group', 'is_add']].sort_values('userid').head(20))
    df.loc[dup_mask].to_csv('./duplicate_users_detail.csv', index=False)
    print("ğŸ“ é‡å¤ç”¨æˆ·è¯¦æƒ…å·²å¯¼å‡º: ./duplicate_users_detail.csv")

    df_no_exact_dups = df.drop_duplicates(keep='first')
    still_dup_mask = df_no_exact_dups.duplicated(subset=['userid'], keep=False)
    conflict_df = df_no_exact_dups[still_dup_mask]
    if not conflict_df.empty:
        print("âŒ åŒä¸€ userid å­˜åœ¨ä¸åŒè®°å½•ï¼ˆå†²çªï¼‰ï¼Œè¯·äººå·¥æ£€æŸ¥ã€‚")
        conflict_df.to_csv('./user_conflicts.csv', index=False)
        raise ValueError("å­˜åœ¨å†²çªç”¨æˆ·è®°å½•ï¼Œåœæ­¢æ‰§è¡Œã€‚")
    else:
        df = df_no_exact_dups
        print("âœ… ä»…å­˜åœ¨å®Œå…¨é‡å¤è¡Œï¼Œå·²è‡ªåŠ¨å»é‡ã€‚")

users_before = check_users(df, "å»é‡å", users_before)

# ==============================
# 3. åˆ†ç»„å”¯ä¸€æ€§ä¸é€»è¾‘æ£€æŸ¥
# ==============================
group_per_user = df.groupby('userid')['group'].nunique()
bad = group_per_user[group_per_user > 1]
if len(bad) > 0:
    bad_users = bad.index.tolist()
    print(f"âŒ æœ‰ {len(bad_users)} ä¸ªç”¨æˆ·å‡ºç°åœ¨å¤šä¸ªåˆ†ç»„: {bad_users[:10]}")
    pd.DataFrame({'bad_userid': bad_users}).to_csv('./conflict_groups.csv', index=False)
    raise ValueError("å®éªŒéšæœºåŒ–è¢«ç ´åï¼šåŒä¸€ç”¨æˆ·å‡ºç°åœ¨å¤šä¸ªç»„ã€‚")

print("âœ… ç”¨æˆ·åˆ†ç»„ä¸€è‡´æ€§é€šè¿‡ã€‚")

# ==============================
# 4. æ•°å€¼ä¸é€»è¾‘åˆæ³•æ€§
# ==============================
num_cols = ['90days_purchase_time','90days_per_purchase_price','90days_purchase_amount',
            '90days_coupon_time','90days_coupon_ratio','last_purchase_day']

for c in num_cols:
    if c in df.columns:
        if (df[c] < 0).any():
            raise ValueError(f"{c} å­˜åœ¨è´Ÿå€¼ã€‚")

mask_invalid = df['90days_coupon_time'] > df['90days_purchase_time']
if mask_invalid.any():
    print(f"âš ï¸ {mask_invalid.sum()} æ¡ä¼˜æƒ åˆ¸æ¬¡æ•°>è´­ä¹°æ¬¡æ•°ï¼Œå·²æ ‡è®°ã€‚")
    df['data_flag'] = np.where(mask_invalid, 'coupon>purchase', 'ok')
else:
    df['data_flag'] = 'ok'

# ==============================
# 5. ç¼ºå¤±å€¼ä¸è¡ç”Ÿç‰¹å¾
# ==============================
for col in ['90days_purchase_time', '90days_per_purchase_price',
            '90days_purchase_amount', '90days_coupon_time', 'last_purchase_day']:
    if col in df.columns:
        df[col] = df[col].fillna(0)

df['90days_coupon_ratio'] = (
    df['90days_coupon_time'] / df['90days_purchase_time'].replace(0, np.nan)
).fillna(0)

df['coupon_per_purchase'] = df['90days_coupon_time'] / df['90days_purchase_time'].replace(0, 1)
df['activity_score'] = 1 / (df['last_purchase_day'] + 1)
df['price_sensitivity'] = df['90days_coupon_ratio'] * df['90days_per_purchase_price']

users_before = check_users(df, "ç‰¹å¾è¡ç”Ÿå", users_before)

# ==============================
# 6. Group ç¼–ç  + OS ç‹¬çƒ­
# ==============================
df['group'] = df['group'].map({
    'control': 0, 'å¯¹ç…§ç»„': 0,
    'exp1': 1, 'å®éªŒç»„1': 1,
    'exp2': 2, 'å®éªŒç»„2': 2
})
if 'user_os' in df.columns:
    df = pd.get_dummies(df, columns=['user_os'], drop_first=True)

# ==============================
# 7. å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆä»…æ ‡è®°ä¸åˆ é™¤ï¼‰
# ==============================
for col in ['90days_purchase_time','90days_purchase_amount','90days_coupon_time']:
    q1, q3 = df[col].quantile([0.25, 0.75])
    iqr = q3 - q1
    upper, lower = q3 + 3*iqr, q1 - 3*iqr
    outliers = df[(df[col] > upper) | (df[col] < lower)]
    if len(outliers) > 0:
        print(f"âš ï¸ {col} æ£€æµ‹åˆ° {len(outliers)} æ¡å¼‚å¸¸å€¼ (æœªåˆ é™¤ï¼Œä»…æ ‡è®°)")
        df.loc[outliers.index, 'data_flag'] = 'outlier'

users_before = check_users(df, "å¼‚å¸¸å€¼æ£€æµ‹å", users_before)

# ==============================
# 8. æ•°å€¼æ ‡å‡†åŒ– + åˆ†ç»„å¹³è¡¡æ£€éªŒ
# ==============================
scaler = StandardScaler()
num_cols_all = ['90days_purchase_time','90days_per_purchase_price','90days_purchase_amount',
                '90days_coupon_time','90days_coupon_ratio','last_purchase_day',
                'coupon_per_purchase','activity_score','price_sensitivity']
df[num_cols_all] = scaler.fit_transform(df[num_cols_all])

report = []
for col in num_cols_all:
    stat, p = stats.kruskal(df[df['group']==0][col],
                            df[df['group']==1][col],
                            df[df['group']==2][col])
    report.append({'feature': col, 'p_value': p})
report_df = pd.DataFrame(report)
report_df['significant_diff'] = report_df['p_value'] < 0.05

# ==============================
# 9. è¾“å‡ºç»“æœ
# ==============================
df.to_csv('./ab_coupon_clean_debug.csv', index=False)
report_df.to_csv('./group_balance_report.csv', index=False)

print("\nâœ… æ•°æ®æ¸…æ´—å®Œæˆ")
print("ğŸ“ æ¸…æ´—å: ./ab_coupon_clean_debug.csv")
print("ğŸ“ åˆ†ç»„å¹³è¡¡æŠ¥å‘Š: ./group_balance_report.csv")
